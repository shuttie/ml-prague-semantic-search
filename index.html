<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Practical fine-tuning for semantic search</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/dracula.css">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section>
				<h2>Practical LLM fine-tuning</h2>
				<img src="img/cat.png" height="350px" style="margin: 0px;">
				<h2>for semantic search</h2>
				<br>
				<small>Roman Grebennikov | ML Prague 2024</small>
			</section>
			<section>
				<h2>Agenda</h2>
				<p>ðŸš€</p>
				<ul>
					<li class="fragment">How does this vector/semantic search work?</li>
					<li class="fragment">Relevance, semantics and intent</li>
					<li class="fragment">Contrastive learning</li>
					<li class="fragment">Negatives: easy and hard</li>
					<li class="fragment">Distillation and transfer</li>
				</ul>
			</section>
			<!--section>
				<h2>whoami</h2>
				<p>ðŸ”Ž</p>
				<ul>
					<li>PhD in CS, quant trading, credit scoring</li>
					<li>Findify: e-commerce search, personalization</li>
					<li>Delivery Hero: food search, LLMs</li>
				</ul>
			</section-->
			<section>
				<h2><strong>Vectors</strong> in vector search</h2>
				<img src="img/embeddings.png" height="400px">
				<ul>
					<li>Embedding: text -> float[] mapping</li>
				</ul>
			</section>
			<section>
				<h2>BERT embeddings</h2>
				<img src="img/bert.png" height="400px">
				<ul>
					<li>Great for masked token prediction</li>
					<li>Not so great for search. Why?</li>
				</ul>
			</section>
			<section>
				<h2>Embedding models for search</h2>
				<img src="img/mteb.png" height="400px">
				<ul>
					<li><strong>MTEB leaderboard</strong>: model eval over 8 tasks</li>
					<li><strong>MTEB retrieval</strong>: search over BEIR dataset collection</li>
				</ul>
			</section>
			<section>
				<h2>BEIR benchmark</h2>
				<img src="img/beir.webp">
				<ul>
					<li>14 datasets, query+positive tuples</li>
					<li>zero-shot: at least in theory</li>
				</ul>
			</section>
			<section>
				<h2><strong>Search</strong> in vector search</h2>
				<img src="img/similarity.png" height="400px">
				<ul>
					<li>Similar query-document pairs have <strong>high similarity score</strong></li>
					<li>Similarity score: <strong>cosine</strong> or dot-product</li>
				</ul>
			</section>
			<section>
				<h2><strong>Search</strong> in vector search</h2>
				<img src="img/similarity.png" height="400px">
				<p><strong>Main perk</strong>: all documents can be embedded offline</p>
			</section>
			<section>
				<h2>Exact vs approximate search</h2>
				<br>
				<p>What if you have 1M products? ðŸ¤”</p>
				<ul>
					<li>Exact: cosine between query and 1M products</li>
					<li>Approx: HNSW, DiskANN - not exact but fast</li>
				</ul>
				<br>
				<p><img src="img/engines.png" height="200px"></p>
			</section>
			<section>
				<p>OK you have embedding and vector search</p>
				<h1>now what?</h1>
			</section>
			<section>
				<h2>Semantics and intent</h2>
				<img src="img/tomato-e5.png" height="500px" style="margin: 0px;">
				<p>"tomato" query: what is relevant?</p>
			</section>
			<section>
				<h2>Semantics and intent</h2>
				<img src="img/tomato-e5-cvr.png" height="500px" style="margin: 0px;">
				<p>Not all tomatos are equal!</p>
			</section>
			<section>
				<h2>Relevance depends on intent?</h2>
				<br>
				<p>Off-the-shelf <strong>zero-shot</strong> embeddings:</p>
				<ul>
					<li>ðŸš€ Great at generalizability: works okey with any data </li>
					<li>ðŸ’” Have no idea about intent of your customers </li>
				</ul>
			</section>
			<section>
				<h2>Do we need to stay <strong>zero-shot</strong>?</h2>
				<ul>
					<li>80% of queries are repeated</li>
					<li>document corpus is mostly the same</li>
				</ul>
				<p><img src="img/tomato-e5-cvr.png" height="300px"></p>
			</section>
			<section>
				<h2>Fine-tuning</h2>
				<p><img src="img/tune.png" height="400px"></p>
				<ul>
					<li>Relevant docs: make them closer to query</li>
					<li>Irrelevant docs: make them further from query</li>
				</ul>
			</section>
			<section>
				<h2>Training</h2>
				<img src="img/sbert.png" height="500px">
			</section>
			<section>
				<h2>Example code</h2>
				<img src="img/sbert-example.png" height="500px">
			</section>
			<section>
				<h2>Data preparation</h2>
				<img src="img/dataprep.png" height="500px">
			</section>
			<section>
				<h2>Example code</h2>
				<img src="img/sbert-example2.png" height="500px">
			</section>
			<section>
				<h2>Base models</h2>
				<img src="img/sbert-models.png" height="350px">
				<ul>
					<li>SBERT pre-trained models</li>
					<li>Models from MTEB leaderboard</li>
					<li>Old good bert-base-uncased</li>
				</ul>
			</section>
			<section>
				<h2>Amazon ESCI: Guinea pig dataset</h2>
				<img src="img/gp.jpg" height="200px">
				<p><a href="github.com/amazon-science/esci-data">github.com/amazon-science/esci-data</a></p>
				<ul>
					<li>48k queries (29k EN, 9 ES, 10 JP)</li>
					<li>1.1M relevance labels: <br><strong>E</strong>xact, <strong>S</strong>ubstitute,
						<strong>C</strong>omplement, <strong>I</strong>rrelevant
					</li>
					<li>Not yet leaked to top models in MTEB leaderboard</li>
				</ul>
			</section>
			<section>
				<h2>Which base model to choose?</h2>
				<p>Experiment: take flavors of <strong>bert-base-uncased</strong> and fine-tune</p>
				<p><img src="img/base.png" height="200px"> </p>
				<p class="fragment"><strong>Learning #1</strong>: pre-trained models are a good starting point</p>
			</section>
			<section>
				<h2>Does size matter?</h2>
				<!--p><img src="img/size.png" height="200px" style="margin: 0px;"></p-->
				<p>Experiment: is <strong>small/base/large</strong> size affects NDCG?</p>
				<p class="fragment"><img src="img/e5.png" height="200px"></p>
				<p class="fragment"><strong>Learning #2</strong>: the larger - the better - the slower</p>
			</section>
			<section>
				<h2>The loss function</h2>
				<p></p>
				<ul>
					<li>Loss function: metric we optimize for</li>
				</ul>
				<p><img src=" img/loss.webp" height="400px" style="margin: 0px;"></p>
				<ul>
					<li>"Yeah let's have NDCG as a loss" - it should be <strong>differentiable</strong></li>
				</ul>
			</section>
			<section>
				<h2>Loss functions</h2>
				<p>SBERT has 24 losses, which one to choose?</p>
				<ul>
					<li><strong>CosineLoss</strong>: query-document-label, float label</li>
					<li><strong>Contrastive</strong>: query-document-label, binary label</li>
					<li><strong>TripletLoss</strong>: query-positive-negative</li>
					<li><strong>MultiNegsRankingLoss</strong>: query-positive</li>
					<li><strong>MultiNegsRankingLoss*</strong>: query-positive-negatives[]</li>
				</ul>
			</section>
			<section>
				<h2>CosineSimilarityLoss</h2>
				<p>Expects query-document-label triplets</p>
				<img src="img/cosine.png" height="300px">
			</section>
			<section>
				<h2>CosineSimilarityLoss</h2>
				<p>But what is label value?</p>
				<p><img src="img/cos90.png" height="400px" style="margin: 0px;"></p>
				<ul>
					<li>Too many orthogonal vectors</li>
				</ul>
			</section>
			<section>
				<h2>CosineSimilarityLoss</h2>
				<p>Cosine scores: expectation vs reality</p>
				<p><img src="img/e5-cos-dist.png" height="350px"></p>
				<ul>
					<li>Expected 0.0, got 0.8 = huge error</li>
				</ul>
			</section>
			<section>
				<h2>Cosine shape depends on training</h2>
				<p><img src="img/cos-temp.png" height="450px"></p>
				<ul><strong>Learning #3</strong>: relevance is a spectrum</ul>
			</section>
			<section>
				<h2>Relevance is a spectrum</h2>
				<p>But what about our training data?</p>
				<p><img src="img/tomato-e5-cvr.png" height="300px"></p>
				<ul>
					<li>Chopped tomatoes 500g</li>
					<li>Makita Power Drill 2KW - more negative?</li>
				</ul>
			</section>
			<section>
				<h2>Negative labels</h2>
				<ul>
					<li>True negative / true positive = ok</li>
					<li>False negative / false positive = not ok</li>
				</ul>
				<p><img src="img/cos-temp-mislabel.png" height="400px"></p>
			</section>
			<section>
				<h2>Contrastive and Triplet losses</h2>
				<ul>
					<li><strong>Contrastive</strong>: query-document-label, binary label</li>
					<li><strong>TripletLoss</strong>: query-positive-negative</li>
				</ul>
				<p><img src="img/triplet.png" height="200px"></p>
				<ul>
					<li>No explicit zero/one labels</li>
					<li>Push positives close, and negatives far</li>
				</ul>
			</section>
			<section>
				<h2>ContrastiveLoss</h2>
				<p><img src="img/contrastive.png" height="550px"></p>
			</section>
			<section>
				<h2>TripletLoss</h2>
				<p><img src="img/triplet2.png" height="550px"></p>
			</section>
			<section>
				<h2>Batch size</h2>
				<p>todo: expetiment cosine+batch</p>
				<ul>Larger batch size = better the NDCG ðŸ¤”</ul>
			</section>
			<section>
				<h2>Let's help Dora find a better way</h2>
				<p><img src="img/triplet2-dora.png" height="550px"></p>
			</section>
			<section>
				<h2>Let's help Dora find a better way</h2>
				<p><img src="img/triplet2-dora.png" height="450px"></p>
				<ul>
					<li>Hint: how many negatives are there for sample #1?</li>
				</ul>
			</section>
			<section>
				<h2>More than one: in-batch negatives!</h2>
				<p><img src="img/triplet-inbatch.png" height="550px"></p>
			</section>
			<section>
				<h2>InfoNCE loss</h2>
				<p><img src="img/infonce-example.png"></p>
				<ul>
					<li>Contrasts query+positive over all in-batch negatives</li>
					<li>Option: contrast over in-batch positives as negatives</li>
				</ul>
			</section>
			<section>
				<h2>InfoNCE vs others</h2>
				<p><img src="img/infonce.png" height="250px"></p>
				<p>Majority of MTEB leaderboard are trained with InfoNCE loss</p>
				<ul><strong>Learning #4</strong>: more data = better</ul>
			</section>
		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,
			history: true,
			controls: true,
			progress: true,
			width: 1200,
			transition: 'none',
			slideNumber: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
		});
	</script>
</body>

</html>